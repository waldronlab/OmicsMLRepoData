---
title: "ETL process for cBioPortal clinical metadata curation/harmonization"
author:
  - Sehyun Oh
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
    html:
        fontsize: 14pxs
        toc: true
        top-depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      collapse = TRUE,
                      eval = FALSE)
```

```{r}
suppressPackageStartupMessages({
    library(googlesheets4)
    library(readr)
    library(OmicsMLRepoR)
    library(dplyr)
})

cbio_etl_dir <- "~/OmicsMLRepo/OmicsMLRepoData/cBioPortalData/ETL"
list.files(cbio_etl_dir)
```


# Curated metadata table
This script collect all the curated metadata table for each attribute and 
create the `curated_all` table, which containing 189,439 sample for 146 newly
created curated columns.

```{r}
source(file.path(cbio_etl_dir, "0_assemble_curated_metadata.R"))
```

```{r save, eval=FALSE}
extDir <- "~/OmicsMLRepo/OmicsMLRepoData/inst/extdata"
write_csv(curated_all, file.path(extDir, "cBioPortal_curated_metadata.csv")) # 189,439 x 160
```


# Format for release
This script creates a `cbio_meta_release` table, which containing only 
the curated attributes formatted in an user-facing version (e.g., no legacy/
source columns, no ontology term id).

```{r}
## requires: `curated_all` from `0_assemble_curated_metadata.R`
## returns: `cbio_meta_release`
source(file.path(cbio_etl_dir, "2_format_for_release.R"))
```

```{r save_cmd_meta_release, eval=FALSE}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
fpath <- file.path(dir, "inst/extdata/cBioPortal_curated_metadata_release.csv")
write.csv(cbio_meta_release, fpath, row.names = FALSE) # 189,439 x 60

## Sync the version in GCS bucket
AnVIL::gsutil_cp(fpath, "gs://omics_ml_repo/cBioPortal_curated_metadata_release.csv")
```


# Sync curation maps
Curation maps are tables manually created and maintained in Google Sheet. We
assign appropriate ontology terms to original values. These tables contain four 
main columns with some additional columns depending on a attribute.

Four main columns are labeled as `original_value`, `curated_ontology`,
`curated_ontology_term_id`, and `curated_ontology_term_db`.

The script below (`1_sync_curation_map.R`) syncs all the cBioPortal curation maps
from Google Sheet to the `cbio_maps_dir` (a directory under the local git 
project).

```{r eval=FALSE}
# Directory to save curation_maps from Google Sheet
cbio_maps_dir <- "~/OmicsMLRepo/OmicsMLRepoData/cBioPortalData/maps"
source(file.path(cbio_etl_dir, "1_sync_curation_map.R")) # Should be updated with new maps
```

```{r maps_updated_through_gh, eval=FALSE}
target_condition <- read.csv(file.path(cbio_maps_dir, "cBioPortal_target_condition_map.csv"))
url <- "https://docs.google.com/spreadsheets/d/1omAoO0N3r3rjBIQuhMB_uJU0WDF6hidg1_qwMTVav2c/edit?usp=sharing"
ss <- googledrive::as_id(url)
googlesheets4::write_sheet(target_condition, ss = ss, sheet = "cBioPortal_target_condition")
```

```{r save_curation_maps, eval=FALSE, echo=FALSE}
write.csv(ancestry_detailed_map, 
          file.path(cbio_maps_dir, "cBioPortal_ancestry_detailed_map.csv"), row.names = FALSE)
write.csv(ancestry_map, file.path(cbio_maps_dir, "cBioPortal_ancestry_map.csv"), row.names = FALSE)
write.csv(bodysite_map, file.path(cbio_maps_dir, "cBioPortal_body_site_map.csv"), row.names = FALSE)
write.csv(country_map, file.path(cbio_maps_dir, "cBioPortal_country_map.csv"), row.names = FALSE)
write.csv(location_map, file.path(cbio_maps_dir, "cBioPortal_location_map.csv"), row.names = FALSE)
write.csv(disease_map, file.path(cbio_maps_dir, "cBioPortal_disease_map.csv"), row.names = FALSE)
write.csv(disease_metastasis_map, file.path(cbio_maps_dir, "cBioPortal_disease_metastasis_map.csv"), row.names = FALSE)
write.csv(vital_status_map, file.path(cbio_maps_dir, "cBioPortal_vital_status_map.csv"), row.names = FALSE)

## Treatments
write.csv(treatment_name_map, file.path(cbio_maps_dir, "cBioPortal_treatment_name_map.csv"), row.names = FALSE)
write.csv(treatment_type_map, file.path(cbio_maps_dir, "cBioPortal_treatment_type_map.csv"), row.names = FALSE)
write.csv(treatment_unit_map, file.path(cbio_maps_dir, "cBioPortal_treatment_unit_map.csv"), row.names = FALSE)
write.csv(treatment_case_map, file.path(cbio_maps_dir, "cBioPortal_treatment_case_map.csv"), row.names = FALSE)
```

```{r save_maps_for_harmonizer, echo=FALSE, eval=FALSE}
## Save maps for harmonizer
attrNames <- c("disease", "treatment_name", "bodysite")
attrFnames <- paste0("cBioPortal_", attrNames, "_map.csv")
harmonizerDataDir <- "~/OmicsMLRepo/OmicsMLRepoHarmonizer/data"

file.copy(file.path(cbio_maps_dir, attrFnames),
          file.path(harmonizerDataDir, attrFnames),
          overwrite = TRUE)
```

# Assemble merging schema
This script calculates the completeness and counts the number of unique 
values for both original and curated fields, and creates the cBioPortal 
merging schema table, `cbio_ms`.

```{r eval=FALSE}
## Need the `curated_all` object from the below script to run `3_assemble_merging_schema_from_data.R`
cbio_etl_dir <- "~/OmicsMLRepo/OmicsMLRepoData/cBioPortalData/ETL"
source(file.path(cbio_etl_dir, "0_assemble_curated_metadata.R"))
```

```{r}
source(file.path(cbio_etl_dir, "3_assemble_merging_schema_from_data.R"))
```

```{r eval=FALSE}
##### GitHub project
## OmicsMLRepoData
extDir <- "~/OmicsMLRepo/OmicsMLRepoData/inst/extdata"
write_csv(cbio_ms, file.path(extDir, "cBioPortal_merging_schema.csv"))

## MetaHarmonizer
hmDir <- "~/OmicsMLRepo/MetaHarmonizer/data"
file.copy(from = file.path(extDir, "cBioPortal_merging_schema.csv"),
          to = file.path(hmDir, "cBioPortal_merging_schema.csv"),
          recursive = TRUE)

##### Google Sheet
url <- "https://docs.google.com/spreadsheets/d/1t2GTvDpgIrR84_ECoft6bQbb2qUr9RoeFtLZWM-ZRDI/edit?usp=sharing"
ss <- googledrive::as_id(url)
googlesheets4::write_sheet(cbio_ms, ss = ss, sheet = "cBioPortal_merging_schema")
```

# Sync per-attribute column compression summary
Because there are quite a lot of columns involved in cBioPortal metadata 
harmonization, its merging schema is created per attribute first. But we didn't
use these per-attribute compression summary for merging schema building. 
```{r}
source(file.path(cbio_etl_dir, "7_sync_compression_summary.R"))
```

```{r save_ms, eval=FALSE}
# Directory to save curation_maps from Google Sheet
cbio_ms_dir <- "~/OmicsMLRepo/OmicsMLRepoData/cBioPortalData/merging_schema/"

write.csv(age_ms, file.path(cbio_ms_dir, "cBioPortal_merging_schema_age.csv"), row.names = FALSE)
write.csv(disease_ms, file.path(cbio_ms_dir, "cBioPortal_merging_schema_disease.csv"), row.names = FALSE)
write.csv(treatment_ms, file.path(cbio_ms_dir, "cBioPortal_merging_schema_treatment.csv"), row.names = FALSE)
```

# Sync data dictionary
This script 'SAVE' data dictionaries.
```{r eval=FALSE}
# cBioPortal data dictionary created in Google Sheet
# Edit this from Google Sheet
source("~/OmicsMLRepo/OmicsMLRepoData/cBioPortalData/ETL/4_sync_data_dictionary.R")
```



# Discrepancies
## Sex
## Population