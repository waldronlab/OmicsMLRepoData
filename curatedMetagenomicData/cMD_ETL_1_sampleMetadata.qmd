---
title: "ETL: cMD sampleMetadata"
author:
  - Sehyun Oh
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
    html:
        fontsize: 14pxs
        toc: true
        top-depth: 3
abstract: "Extract, Transform, and Load (ETL): We create a map between original curation term and their corresponding ontology terms. This 'map' is applied to the sampleMetadata table to update the previously curated cMD sampleMetadata to follow the new, ontology-containing schema."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      collapse = TRUE)
```

```{r echo=FALSE}
suppressPackageStartupMessages({
    library(dplyr)
    library(googlesheets4)
    library(readr)
})
```

# Update curation map
Since the initial curation mapping (i.e., 'original term - ontology term' 
mapping) is done, any additional updates are applied directly to the map 
(the Google Sheet version) and the data files (the csv file version). 

Here is the example of updating map:

```{r eval=FALSE}
## Connect to the cMD_curation table in Google Drive
url <- "https://docs.google.com/spreadsheets/d/1QSbB_b1DkfqOc7q5eHE0IDHSiGqNUyTE8d4GzbSEzjM/edit?usp=sharing"
ss <- googledrive::as_id(url)

## Import the curation map from Google Sheet to update GitHub repo version
map <- read_sheet(ss, sheet = "disease_ontology_all")

## Save the updated version in the local GitHub branch
write.csv(map, file = "maps/cMD_study_condition_ontology.csv", row.names = FALSE)

## Push the change to the GitHub repo
```

```{r updating_maps, echo=FALSE, eval=FALS}
## Same as the above chunk
url <- "https://docs.google.com/spreadsheets/d/1QSbB_b1DkfqOc7q5eHE0IDHSiGqNUyTE8d4GzbSEzjM/edit?usp=sharing"
ss <- googledrive::as_id(url)

attribute <- "treatment"
map <- read_sheet(ss, sheet = paste0(attribute, "_ontology"))
write.csv(map, file = paste0("maps/cMD_", attribute, "_ontology.csv"), row.names = FALSE)
```


# Update curated sampleMetadata
Curation/mapping process of a given attributes are documented in the `.qmd`
files under `curatedMetagenomicData/` directory. Please find and run proper
qmd files to update the curation data.

## Import curated/harmonized attributes
```{r}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
dataDir <- file.path(dir, "curatedMetagenomicData/data")
curated <- list.files(dataDir)
curated <- curated[grepl("\\.csv$", curated)] # choose only file
curatedDatObj <- gsub(".csv", "", curated)

for (i in seq_along(curated)) {
    res <- read_csv(file.path(dataDir, curated[i]))
    assign(curatedDatObj[i], res)
}
```

All the column names of the curated sample metadata per attributes
```{r colname_sanity_check, echo=FALSE}
sapply(gsub(".csv", "", curated), function(x) {colnames(get(x))})
```

## Combine all the curated sample metadata
```{r}
curated_all <- get(curatedDatObj[1])
for (i in 2:length(curatedDatObj)) { # assuming there are >= 2 tables to be combined
    dat <- get(curatedDatObj[i])
    curated_all <- left_join(curated_all, dat, by = "curation_id")
}
```

## Combine original and curated sample metadata
Combine the original sampleMetadata and curated columns
```{r original_sampleMetadata, message=FALSE}
## [Update needed] to get data files directly from the GH repo
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
ori <- read_csv(file.path(dir, "inst/extdata/cMD_sampleMetadata.csv"))

allSampleMetadata <- ori %>%
    mutate(curation_id = paste(study_name, sample_id, sep = ":")) %>%
    dplyr::left_join(., curated_all, by = "curation_id")

allSampleMetadata$last_modified <- Sys.Date() #<<< Can I check this at the sample-level?

##### Update the column names
merging_schema <- read_csv(file.path(dir, "inst/extdata/cMD_merging_schema.csv"))

## Names of the original fields
old_fields <- sapply(merging_schema$original_fields, strsplit, split = ";") %>% 
    unlist %>% 
    as.character
old_fields <- old_fields[!is.na(old_fields)]

## Names of the new fields
new_fields <- sapply(merging_schema$curated_fields, strsplit, split = ";") %>% 
    unlist %>% 
    as.character

## Add prefix `legacy_` for the legacy columns
legacy_ind <- which(colnames(allSampleMetadata) %in% old_fields)
colnames(allSampleMetadata)[legacy_ind] <- paste0("legacy_", colnames(allSampleMetadata)[legacy_ind])
```

```{r}
# ## Remove columns start with `original_`
# updated <- allSampleMetadata %>% 
#     select(!starts_with("original_"))
# 
# ## Update column names for `curated_` columns (remove prefix in the names)
# name1 <- colnames(allSampleMetadata)[grep("^curated_", colnames(allSampleMetadata))]
# name2 <-  gsub("^curated_", "", name1)
# data.table::setnames(updated, old = name1, new = name2)

curated_ind <- grep("^curated_", colnames(allSampleMetadata))
name1 <- colnames(allSampleMetadata)[curated_ind]
name2 <-  gsub("^curated_", "", name1)

updated <- relocate(allSampleMetadata, starts_with("curated_"), .after = subject_id)
updated <- relocate(updated, starts_with("legacy_"), .before = last_modified)

updated <- updated %>%
    select(!starts_with("original_"))

data.table::setnames(updated, old = name1, new = name2)
sum(duplicated(colnames(updated))) # should be 0
```

The `updated` table has all the additional/updated columns after the three 
id columns (`study_name`, `sample_id`, `subject_id`) at the beginning and
the original columns with the `legacy_` prefix at the end before the 
`last_modified` column.



# Save
## Full version
The `cMD_curated_sampleMetadata.csv` will be a direct replacement of the 
original `sampleMetadata.tsv` because it still contains all the original 
columns. Currently new columns 

```{r}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
fpath <- file.path(dir, "inst/extdata/cMD_curated_sampleMetadata.csv")
write.csv(updated, fpath, row.names = FALSE)
```

## Compact version
```{r}
source("R/compact_metadata.R")
dim(meta)
```

```{r}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
fpath_compact <- file.path(dir, "inst/extdata/cMD_curated_compact_sampleMetadata.csv")
write.csv(meta, fpath_compact, row.names = FALSE)
```

## Parquet for DuckDB
```{r message=FALSE}
allSampleMetadata <- read_csv(fpath) # For now, use complete, non-compact version of the sampleMetadata
parquet <- gsub(".csv$", ".parquet", fpath)
arrow::write_parquet(allSampleMetadata, 
                     sink = parquet)
AnVIL::gsutil_cp(parquet,
                 dest = "gs://omics_ml_repo/cMD_curated_sampleMetadata.parquet")
```




