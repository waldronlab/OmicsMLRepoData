---
title: "ETL: cMD sampleMetadata datable"
author:
  - Sehyun Oh
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
    html:
        fontsize: 14pxs
        toc: true
        top-depth: 3
abstract: "Extract, Transform, and Load (ETL): We create a map between original curation term and their corresponding ontology terms. This 'map' is applied to the sampleMetadata table to update the previously curated cMD sampleMetadata to follow the new, ontology-containing schema."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      collapse = TRUE)
```

```{r echo=FALSE}
suppressPackageStartupMessages({
    library(dplyr)
    library(googlesheets4)
})
```

# Update curation map
Since the initial curation mapping (i.e., 'original term - ontology term' 
mapping) is done, any additional updates are applied directly to the map 
(the Google Sheet version) and the data files (the csv file version). 

Here is the example of updating map:

```{r eval=FALSE}
## Connect to the cMD_curation table in Google Drive
url <- "https://docs.google.com/spreadsheets/d/1QSbB_b1DkfqOc7q5eHE0IDHSiGqNUyTE8d4GzbSEzjM/edit?usp=sharing"
ss <- googledrive::as_id(url)

## Import the curation map from Google Sheet to update GitHub repo version
map <- read_sheet(ss, sheet = "disease_ontology_all")

## Save the updated version in the local GitHub branch
write.csv(map, file = "maps/cMD_study_condition_ontology.csv", row.names = FALSE)

## Push the change to the GitHub repo
```

# Update curated sampleMetadata
Curation/mapping process of a given attributes are documented in the `.qmd`
files under `curatedMetagenomicData/` directory. Please find and run proper
qmd files to update the curation data.

## Import curated/harmonized attributes
```{r}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
dataDir <- file.path(dir, "curatedMetagenomicData/data")
curated <- list.files(dataDir)
curatedDatObj <- gsub(".csv", "", curated)

for (i in seq_along(curated)) {
    res <- read.csv(file.path(dataDir, curated[i]))
    assign(curatedDatObj[i], res)
}
```

All the column names of the curated sample metadata per attributes
```{r colname_sanity_check, echo=FALSE}
sapply(gsub(".csv", "", curated), function(x) {colnames(get(x))})
```

## Combine all the curated sample metadata
```{r}
curated_all <- get(curatedDatObj[1])
for (i in 2:length(curatedDatObj)) { # assuming there are >= 2 tables to be combined
    dat <- get(curatedDatObj[i])
    curated_all <- left_join(curated_all, dat, by = "curation_id")
}
```

### Combine original and curated
Combine the original sampleMetadata and curated columns
```{r original_sampleMetadata}
## [Update needed] to get data files directly from the GH repo
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
ori <- read.csv(file.path(dir, "inst/extdata/cMD_sampleMetadata.csv"))

allSampleMetadata <- ori %>%
    mutate(curation_id = paste(study_name, sample_id, sep = ":")) %>%
    dplyr::left_join(., curated_all, by = "curation_id")

allSampleMetadata$last_modified <- Sys.Date() #<<< Can I check this at the sample-level?
```

# Save
## Full version
The `cMD_curated_sampleMetadata.csv` will be a direct replacement of the 
original `sampleMetadata.tsv` because it still contains all the original 
columns. Currently new columns 

```{r save, eval=FALSE}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
fpath <- file.path(dir, "inst/extdata/cMD_curated_sampleMetadata.csv")
write.csv(allSampleMetadata, 
          fpath,
          row.names = FALSE)
```

## Compact version
```{r}
source("R/compact_metadata.R")
dim(meta)
```

```{r}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
fpath_compact <- file.path(dir, "inst/extdata/cMD_curated_compact_sampleMetadata.csv")
write.csv(meta, fpath_compact, row.names = FALSE)
```

## Parquet for DuckDB
```{r eval=FALSE}
allSampleMetadata <- read.csv(fpath) # For now, use complete, non-compact version of the sampleMetadata
parquet <- gsub(".csv$", ".parquet", fpath)
arrow::write_parquet(allSampleMetadata, 
                     sink = parquet)
AnVIL::gsutil_cp(parquet,
                 dest = "gs://omics_ml_repo/cMD_curated_sampleMetadta.parquet")
```




# Completeness update
```{r}
dir <- "~/OmicsMLRepo/OmicsMLRepoData"
fpath <- file.path(dir, "inst/extdata/cMD_curated_sampleMetadata.csv")
allSampleMetadata <- read.csv(fpath)
```

```{r import_gs, eval=FALSE}
url <- "https://docs.google.com/spreadsheets/d/1xziFB_zBl32BjNarcyEN4GupTYpPtq5aDz0GbRbWvtk/edit?usp=sharing"
ss <- googledrive::as_id(url)
merging_schema <- googlesheets4::read_sheet(ss = ss, sheet = "merging_schema")
```

From the harmonization/curation merging_schema table, which contains the original
fields and their curated versions as listed in their column names.

```{r cal_completness_and_report, eval=FALSE}
library(OmicsMLRepoCuration)
original_fields_completeness <- checkCurationStats(
    fields_list = merging_schema$original_fields,
    DB = "cMD")
original_fields_unique_vals <- checkCurationStats(
    fields_list = merging_schema$original_fields,
    check = "unique",
    DB = "cMD")

x <- merging_schema$curated_fields 
curated_fields_name <- sapply(x, function(x) {
    strsplit(x, split = ";") %>% 
        unlist %>% 
        # paste0("curated_", .) %>% 
        paste0(., collapse = ";")
}) %>% as.vector

curated_fields_completeness <- checkCurationStats(
    fields_list = curated_fields_name,
    DB = "cMD")
curated_fields_unique_vals <- checkCurationStats(
    fields_list = curated_fields_name,
    check = "unique",
    DB = "cMD")
```

```{r eval=FALSE}
merging_schema$original_fields_completeness <- original_fields_completeness
merging_schema$curated_fields_completeness <- curated_fields_completeness
merging_schema$original_fields_unique_vals <- original_fields_unique_vals
merging_schema$curated_fields_unique_vals <- curated_fields_unique_vals

merging_schema <- merging_schema %>% 
    # dplyr::relocate(original_fields_completeness, .after = original_fields) %>%
    # dplyr::relocate(curated_fields_completeness, .after = curated_fields) %>%
    dplyr::relocate(original_fields_unique_vals, .after = original_fields_completeness) %>%
    dplyr::relocate(curated_fields_unique_vals, .after = curated_fields_completeness)
```

```{r save_as_Google_Sheets, echo=FALSE, eval=FALSE}
googlesheets4::write_sheet(merging_schema, ss = ss, sheet = "merging_schema")
```

